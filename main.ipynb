{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63504c16",
   "metadata": {},
   "source": [
    "# Les imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55a9af7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mT\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af162f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9477d3f",
   "metadata": {},
   "source": [
    "check GPU stats :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d5f3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a51a38b",
   "metadata": {},
   "source": [
    "# Los geht's !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02f04c2",
   "metadata": {},
   "source": [
    "### Definition des classes d'objets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb66245",
   "metadata": {},
   "source": [
    "#### Classe de nos datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f312d97",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mGeoEye1\u001b[39;00m(\u001b[43mDataset\u001b[49m):\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# mapping between label class names and indices\u001b[39;00m\n\u001b[32m      4\u001b[39m     LABEL_CLASSES = {\n\u001b[32m      5\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mno_damage\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0\u001b[39m,\n\u001b[32m      6\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mdamage\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m1\u001b[39m,\n\u001b[32m      7\u001b[39m     }\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, root_dir=\u001b[33m\"\u001b[39m\u001b[33mdata/ipeo_hurricane_for_students\u001b[39m\u001b[33m\"\u001b[39m, split=\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m, transforms=\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[31mNameError\u001b[39m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class GeoEye1(Dataset):\n",
    "\n",
    "    # mapping between label class names and indices\n",
    "    LABEL_CLASSES = {\n",
    "        'no_damage': 0,\n",
    "        'damage': 1,\n",
    "    }\n",
    "\n",
    "    def __init__(self, root_dir=\"data/ipeo_hurricane_for_students\", split='train', transforms=None):\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # Chemin du dossier correspondant au split (train/test/validation)\n",
    "        split_dir = os.path.join(root_dir, split)\n",
    "\n",
    "        # Liste qui contiendra (chemin_image, label)\n",
    "        self.data = []\n",
    "\n",
    "        # Pour chaque classe (damage / no_damage)\n",
    "        for class_name, class_idx in self.LABEL_CLASSES.items():\n",
    "            class_dir = os.path.join(split_dir, class_name)\n",
    "\n",
    "            if not os.path.isdir(class_dir):\n",
    "                continue\n",
    "\n",
    "            # Récupération de toutes les images du dossier\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                if img_name.lower().endswith(\".jpeg\"):\n",
    "                    img_path = os.path.join(class_dir, img_name)\n",
    "                    self.data.append((img_path, class_idx))\n",
    "\n",
    "        # Optionnel: trier pour reproductibilité\n",
    "        self.data.sort()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41618a71",
   "metadata": {},
   "source": [
    "##### Calculs préliminaires de la mean et std des train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db1dd4b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'T' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Dataset sans augmentation et sans Normalize pour calculer mean/std\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m transforms_for_stats = \u001b[43mT\u001b[49m.Compose([\n\u001b[32m      3\u001b[39m     T.Resize((\u001b[32m224\u001b[39m, \u001b[32m224\u001b[39m)),\n\u001b[32m      4\u001b[39m     T.ToTensor()\n\u001b[32m      5\u001b[39m ])\n\u001b[32m      7\u001b[39m train_dataset_raw = GeoEye1(\n\u001b[32m      8\u001b[39m     root_dir=\u001b[33m\"\u001b[39m\u001b[33mdata/ipeo_hurricane_for_students\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m     split=\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m     transforms=transforms_for_stats\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m loader = DataLoader(train_dataset_raw, batch_size=\u001b[32m32\u001b[39m, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers=\u001b[32m4\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'T' is not defined"
     ]
    }
   ],
   "source": [
    "# Dataset sans augmentation et sans Normalize pour calculer mean/std\n",
    "transforms_for_stats = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset_raw = GeoEye1(\n",
    "    root_dir=\"data/ipeo_hurricane_for_students\",\n",
    "    split=\"train\",\n",
    "    transforms=transforms_for_stats\n",
    ")\n",
    "\n",
    "loader = DataLoader(train_dataset_raw, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# Compute mean & std\n",
    "mean = 0.\n",
    "std = 0.\n",
    "total_images = 0\n",
    "\n",
    "for imgs, _ in loader:\n",
    "    batch_size = imgs.size(0)\n",
    "    imgs = imgs.view(batch_size, imgs.size(1), -1)\n",
    "    mean += imgs.mean(2).sum(0)\n",
    "    std += imgs.std(2).sum(0)\n",
    "    total_images += batch_size\n",
    "\n",
    "mean /= total_images\n",
    "std /= total_images\n",
    "\n",
    "print(\"Mean =\", mean)\n",
    "print(\"Std  =\", std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48a529c",
   "metadata": {},
   "source": [
    "#### preprocess des images, data augmentation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f745f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = T.Normalize(mean, std)\n",
    "\n",
    "std_inv = 1 / (std + 1e-7)\n",
    "unnormalize = T.Normalize(-mean * std_inv, std_inv)\n",
    "\n",
    "transforms_train = T.Compose([\n",
    "  T.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "  T.RandomGrayscale(p=0.7),\n",
    "  T.RandomHorizontalFlip(p=0.8),\n",
    "  T.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5.0)),\n",
    "  T.RandomPosterize(bits=6, p=0.5),\n",
    "  T.RandomVerticalFlip(p=0.5),\n",
    "  T.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5 , hue=0.3),\n",
    "  T.Resize((224, 224)),\n",
    "  T.ToTensor(),\n",
    "  normalize\n",
    "])\n",
    "\n",
    "transforms_val = T.Compose([\n",
    "  T.Resize((224, 224)),\n",
    "  T.ToTensor(),\n",
    "  normalize\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eeab97",
   "metadata": {},
   "source": [
    "#### Visualiser quelques images du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4431618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset original (sans augmentations)\n",
    "raw_ds = GeoEye1(\"data/ipeo_hurricane_for_students\", \"train\",\n",
    "                 transforms=T.Compose([T.Resize((224,224)), T.ToTensor()]))\n",
    "\n",
    "# dataset avec augmentations\n",
    "aug_ds = GeoEye1(\"data/ipeo_hurricane_for_students\", \"train\",\n",
    "                 transforms=transforms_train)\n",
    "\n",
    "def show_tensor_image(t):\n",
    "    img = unnormalize(t).clamp(0,1).permute(1,2,0).cpu().numpy()\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "for i in range(4):\n",
    "    raw_img, _ = raw_ds[i]\n",
    "    aug_img, _ = aug_ds[i]\n",
    "\n",
    "    plt.subplot(4,2,2*i+1)\n",
    "    plt.title(\"Brute\")\n",
    "    show_tensor_image(raw_img)\n",
    "\n",
    "    plt.subplot(4,2,2*i+2)\n",
    "    plt.title(\"Transformée\")\n",
    "    show_tensor_image(aug_img)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc36c625",
   "metadata": {},
   "source": [
    "a `torch.utils.data.DataLoader` is a wrapper around a `torch.utils.data.Dataset` class that allows us to load multiple images in a batch **in parallel** (on CPU and RAM) which speeds up the training by loading data faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8ee41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"class LightningClassifierModelWrapper(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        acc = (y_hat.argmax(1) == y).float().mean()\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_accuracy\", acc, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(self.parameters(), lr=0.01, momentum=0.9)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abbe6d2",
   "metadata": {},
   "source": [
    "### Instanciation des objets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6db6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model = torchvision.models.resnet18(num_classes=2)\\npytorch_lightning_model = LightningClassifierModelWrapper(model)\\n\\ntb_logger = pl_loggers.TensorBoardLogger(save_dir=\"logs/\", name=\"experimentname\")\\n\\ncheckpoint_callback = ModelCheckpoint(\\n    dirpath=\\'checkpoints\\',\\n    filename=\\'alexnet-{epoch}-{val_accuracy:.2f}\\',\\n    monitor=\"val_accuracy\",\\n    mode=\"max\"\\n    )\\n\\ntrainer = pl.Trainer(max_epochs=100, accelerator=\"cpu\", devices=1,        # replace by accelerator=\"gpu\", devices=[0]\\n                     logger=tb_logger, callbacks=[checkpoint_callback])'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"train_dataset = GeoEye1(\"data/ipeo_hurricane_for_students\", \"train\", transforms_train)\n",
    "val_dataset = GeoEye1(\"data/ipeo_hurricane_for_students\", \"validation\", transforms_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=8)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "\n",
    "model = torchvision.models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
    "\n",
    "lightning_model = LightningClassifierModelWrapper(model)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    logger=pl.loggers.TensorBoardLogger(\"logs/\", name=\"hurricane\"),\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(\n",
    "            dirpath=\"checkpoints\",\n",
    "            filename=\"resnet18-{epoch}-{val_accuracy:.2f}\",\n",
    "            monitor=\"val_accuracy\",\n",
    "            mode=\"max\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0ccfde",
   "metadata": {},
   "source": [
    "### Appel des fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb2aa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"trainer.fit(lightning_model, train_loader, val_loader)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
