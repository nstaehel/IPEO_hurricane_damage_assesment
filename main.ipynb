{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63504c16",
   "metadata": {},
   "source": [
    "# Les imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55a9af7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af162f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9477d3f",
   "metadata": {},
   "source": [
    "check GPU stats :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07d5f3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvidia-smi' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a51a38b",
   "metadata": {},
   "source": [
    "# Los geht's !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02f04c2",
   "metadata": {},
   "source": [
    "### Definition des classes d'objets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb66245",
   "metadata": {},
   "source": [
    "#### Classe de nos datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4097d36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f312d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoEye1(Dataset):\n",
    "\n",
    "    # mapping between label class names and indices\n",
    "    LABEL_CLASSES = {\n",
    "        'no_damage': 0,\n",
    "        'damage': 1,\n",
    "    }\n",
    "\n",
    "    def __init__(self, root_dir=\"data/ipeo_hurricane_for_students\", split='train', transforms=None):\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # Chemin du dossier correspondant au split (train/test/validation)\n",
    "        split_dir = os.path.join(root_dir, split)\n",
    "\n",
    "        # Liste qui contiendra (chemin_image, label)\n",
    "        self.data = []\n",
    "\n",
    "        # Pour chaque classe (damage / no_damage)\n",
    "        for class_name, class_idx in self.LABEL_CLASSES.items():\n",
    "            class_dir = os.path.join(split_dir, class_name)\n",
    "\n",
    "            if not os.path.isdir(class_dir):\n",
    "                continue\n",
    "\n",
    "            # Récupération de toutes les images du dossier\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                if img_name.lower().endswith(\".jpeg\"):\n",
    "                    img_path = os.path.join(class_dir, img_name)\n",
    "                    self.data.append((img_path, class_idx))\n",
    "\n",
    "        # Optionnel: trier pour reproductibilité\n",
    "        self.data.sort()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41618a71",
   "metadata": {},
   "source": [
    "##### Calculs préliminaires de la mean et std des train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db1dd4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset sans augmentation et sans Normalize pour calculer mean/std\n",
    "transforms_for_stats = T.Compose([\n",
    "    T.Resize((150, 150)),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset_raw = GeoEye1(\n",
    "    root_dir=\"data/ipeo_hurricane_for_students\",\n",
    "    split=\"train\",\n",
    "    transforms=transforms_for_stats\n",
    ")\n",
    "\n",
    "loader = DataLoader(train_dataset_raw, batch_size=56, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a4c1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean & std\n",
    "mean = 0.\n",
    "std = 0.\n",
    "total_images = 0\n",
    "\n",
    "for imgs, _ in loader:\n",
    "    batch_size = imgs.size(0)\n",
    "    imgs = imgs.view(batch_size, imgs.size(1), -1)\n",
    "    mean += imgs.mean(2).sum(0)\n",
    "    std += imgs.std(2).sum(0)\n",
    "    total_images += batch_size\n",
    "\n",
    "mean /= total_images\n",
    "std /= total_images\n",
    "\n",
    "print(\"Mean =\", mean)\n",
    "print(\"Std  =\", std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c365d216",
   "metadata": {},
   "source": [
    "avec images 224*224:\\\n",
    "Mean = tensor([0.3521, 0.3835, 0.2852])\\\n",
    "Std  = tensor([0.1132, 0.0980, 0.0992])\n",
    "\n",
    "avec images 150*150:\\\n",
    "Mean = tensor([0.3518, 0.3832, 0.2850])\n",
    "Std  = tensor([0.1127, 0.0974, 0.0986])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48a529c",
   "metadata": {},
   "source": [
    "#### preprocess des images, data augmentation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f745f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "std  = torch.tensor([0.1127, 0.0974, 0.0986])\n",
    "mean = torch.tensor([0.3518, 0.3832, 0.2850])\n",
    "\n",
    "normalize = T.Normalize(mean, std)\n",
    "\n",
    "std_inv = 1 / (std + 1e-7)\n",
    "unnormalize = T.Normalize(-mean * std_inv, std_inv)\n",
    "\n",
    "transforms_train = T.Compose([\n",
    "  T.RandomResizedCrop(size=(150, 150), antialias=True),\n",
    "  T.RandomHorizontalFlip(p=0.1),\n",
    "  T.RandomVerticalFlip(p=0.1),\n",
    "  T.RandomPosterize(bits=6, p=0.1),\n",
    "  T.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5 , hue=0.3),\n",
    "  #T.RandomAffine(degrees=(0, 180), translate=None, scale=None, shear=None, interpolation='nearest', fill=0),\n",
    "\n",
    "  T.Resize((150, 150)),\n",
    "  T.ToTensor(),\n",
    "  normalize\n",
    "])\n",
    "\n",
    "transforms_val = T.Compose([\n",
    "  T.Resize((150, 150)),\n",
    "  T.ToTensor(),\n",
    "  normalize\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eeab97",
   "metadata": {},
   "source": [
    "#### Visualiser quelques images du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4431618b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      2\u001b[39m raw_ds = GeoEye1(\u001b[33m\"\u001b[39m\u001b[33mdata/ipeo_hurricane_for_students\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m                  transforms=T.Compose([T.Resize((\u001b[32m150\u001b[39m,\u001b[32m150\u001b[39m)), T.ToTensor()]))\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# dataset avec augmentations\u001b[39;00m\n\u001b[32m      6\u001b[39m aug_ds = GeoEye1(\u001b[33m\"\u001b[39m\u001b[33mdata/ipeo_hurricane_for_students\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m                  transforms=\u001b[43mtransforms_train\u001b[49m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow_tensor_image\u001b[39m(t):\n\u001b[32m     10\u001b[39m     img = t.clamp(\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m).permute(\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m,\u001b[32m0\u001b[39m).cpu().numpy()\n",
      "\u001b[31mNameError\u001b[39m: name 'transforms_train' is not defined"
     ]
    }
   ],
   "source": [
    "# dataset original (sans augmentations)\n",
    "raw_ds = GeoEye1(\"data/ipeo_hurricane_for_students\", \"train\",\n",
    "                 transforms=T.Compose([T.Resize((150,150)), T.ToTensor()]))\n",
    "\n",
    "# dataset avec augmentations\n",
    "aug_ds = GeoEye1(\"data/ipeo_hurricane_for_students\", \"train\",\n",
    "                 transforms=transforms_train)\n",
    "\n",
    "def show_tensor_image(t):\n",
    "    img = t.clamp(0,1).permute(1,2,0).cpu().numpy()\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "for i in range(4):\n",
    "    raw_img, _ = raw_ds[i]\n",
    "    aug_img, _ = aug_ds[i]\n",
    "\n",
    "    plt.subplot(4,2,2*i+1)\n",
    "    plt.title(\"Brute\")\n",
    "    show_tensor_image(raw_img)\n",
    "\n",
    "    plt.subplot(4,2,2*i+2)\n",
    "    plt.title(\"Transformée\")\n",
    "    show_tensor_image(unnormalize(aug_img))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc36c625",
   "metadata": {},
   "source": [
    "a `torch.utils.data.DataLoader` is a wrapper around a `torch.utils.data.Dataset` class that allows us to load multiple images in a batch **in parallel** (on CPU and RAM) which speeds up the training by loading data faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8ee41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningClassifierModelWrapper(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        acc = (y_hat.argmax(1) == y).float().mean()\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_accuracy\", acc, prog_bar=True)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        return y_hat, y\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # return torch.optim.Adam(self.parameters(), lr=0.01, momentum=0.9)\n",
    "        return torch.optim.SGD(self.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abbe6d2",
   "metadata": {},
   "source": [
    "### Instanciation des objets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d8d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GeoEye1(\"ipeo_hurricane_for_students\", \"train\", transforms_train)\n",
    "val_dataset = GeoEye1(\"ipeo_hurricane_for_students\", \"validation\", transforms_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f97ca1",
   "metadata": {},
   "source": [
    "ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6db6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model = torchvision.models.resnet18(num_classes=2)\\npytorch_lightning_model = LightningClassifierModelWrapper(model)\\n\\ntb_logger = pl_loggers.TensorBoardLogger(save_dir=\"logs/\", name=\"experimentname\")\\n\\ncheckpoint_callback = ModelCheckpoint(\\n    dirpath=\\'checkpoints\\',\\n    filename=\\'alexnet-{epoch}-{val_accuracy:.2f}\\',\\n    monitor=\"val_accuracy\",\\n    mode=\"max\"\\n    )\\n\\ntrainer = pl.Trainer(max_epochs=100, accelerator=\"cpu\", devices=1,        # replace by accelerator=\"gpu\", devices=[0]\\n                     logger=tb_logger, callbacks=[checkpoint_callback])'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toujours de l'instanciation des objet...\n",
    "model = torchvision.models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "#model = torchvision.models.alexnet(weights=\"IMAGENET1K_V1\")\n",
    "#model = DEFINIR NOTRE PROPRE MODELE\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
    "\n",
    "lightning_model = LightningClassifierModelWrapper(model)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    logger=pl.loggers.TensorBoardLogger(\"logs/\", name=\"hurricane\"),\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(\n",
    "            dirpath=\"checkpoints\",\n",
    "            filename=\"resnet18-{epoch}-{val_accuracy:.2f}\",\n",
    "            monitor=\"val_accuracy\",\n",
    "            mode=\"max\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0ccfde",
   "metadata": {},
   "source": [
    "### Appel des fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb2aa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(lightning_model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933759ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c6a6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b78688b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = GeoEye1(transforms = transforms_val, split=\"test\")\n",
    "test_dl = DataLoader(test_dataset, batch_size = 16)\n",
    "\n",
    "# TODO: replace with the checkpoint of your trained model\n",
    "checkpoint_path = \"checkpoints/resnet18-epoch=3-val_accuracy=0.92.ckpt\"\n",
    "#checkpoint_path = \"checkpoints/alexnet-epoch=48-val_accuracy=0.70.ckpt\"\n",
    "\n",
    "rs = trainer.predict(lightning_model, dataloaders=test_dl,\n",
    "                ckpt_path=checkpoint_path)\n",
    "\n",
    "y_hat, y = list(map(list, zip(*rs)))\n",
    "y_hat = torch.vstack(y_hat)\n",
    "y = torch.hstack(y)\n",
    "accuracy = (y_hat.argmax(1) == y).float().mean()\n",
    "print(f\"test accuracy {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd0e7bf",
   "metadata": {},
   "source": [
    "## Model Calibration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2220e22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class IsotonicCalibration:\n",
    "    \"\"\"\n",
    "    Isotonic Regression pour calibrer les probabilités du modèle\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.iso_reg = IsotonicRegression(y_min=0,y_max=1,out_of_bounds='clip')\n",
    "    \n",
    "    def fit(self, logits, labels):\n",
    "        \"\"\"\n",
    "        Entraîne le modèle de calibration sur le validation set\n",
    "        \"\"\"\n",
    "        # Convertir logits en probabilités\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        # Prendre la probabilité de la classe prédite\n",
    "        confidences = torch.max(probs, dim=1)[0].numpy()\n",
    "        \n",
    "        # Créer des targets binaires (1 si prédiction correcte, 0 sinon)\n",
    "        predictions = torch.argmax(probs, dim=1)\n",
    "        correct = (predictions == labels).float().numpy()\n",
    "        \n",
    "        # Fit isotonic regression\n",
    "        self.iso_reg.fit(confidences, correct)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, logits):\n",
    "        \"\"\"\n",
    "        Retourne les probabilités calibrées\n",
    "        \"\"\"\n",
    "        probs = F.softmax(logits, dim=1).numpy()\n",
    "        \n",
    "        # Calibrer chaque probabilité\n",
    "        calibrated_probs = np.zeros_like(probs)\n",
    "        for i in range(probs.shape[0]):\n",
    "            for j in range(probs.shape[1]):\n",
    "                calibrated_probs[i, j] = self.iso_reg.predict([probs[i, j]])[0]\n",
    "        \n",
    "        # Normaliser pour que la somme = 1\n",
    "        calibrated_probs = calibrated_probs / calibrated_probs.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        return torch.from_numpy(calibrated_probs).float()\n",
    "\n",
    "def compute_ece(probs, labels, n_bins=15):\n",
    "    \"\"\"\n",
    "    Expected Calibration Error\n",
    "    Mesure la différence entre confiance et précision\n",
    "    \"\"\"\n",
    "    confidences, predictions = torch.max(probs, 1)\n",
    "    accuracies = predictions.eq(labels)\n",
    "    \n",
    "    bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
    "    ece = torch.zeros(1)\n",
    "    \n",
    "    for bin_lower, bin_upper in zip(bin_boundaries[:-1], bin_boundaries[1:]):\n",
    "        in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
    "        prop_in_bin = in_bin.float().mean()\n",
    "        \n",
    "        if prop_in_bin.item() > 0:\n",
    "            accuracy_in_bin = accuracies[in_bin].float().mean()\n",
    "            avg_confidence_in_bin = confidences[in_bin].mean()\n",
    "            ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "    \n",
    "    return ece.item()\n",
    "\n",
    "def collect_logits_and_labels(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Collecte tous les logits et labels du validation set\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            logits = model(images)\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_labels.append(labels)\n",
    "    \n",
    "    return torch.cat(all_logits), torch.cat(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ca7208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation de la calibration\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lightning_model.to(device)\n",
    "\n",
    "# 1. Collecter les logits sur le validation set\n",
    "print(\"Collecte des logits...\")\n",
    "val_logits, val_labels = collect_logits_and_labels(\n",
    "    lightning_model.model, \n",
    "    val_loader, \n",
    "    device\n",
    ")\n",
    "\n",
    "# 2. Calculer l'ECE avant calibration\n",
    "nb_bins = 15\n",
    "probs_before = F.softmax(val_logits, dim=1)\n",
    "ece_before = compute_ece(probs_before, val_labels, n_bins=nb_bins)\n",
    "print(f\"ECE avant calibration: {ece_before:.4f}\")\n",
    "\n",
    "# 3. Entraîner le modèle de calibration\n",
    "print(\"\\nCalibration du modèle...\")\n",
    "Iso_cal = IsotonicCalibration()\n",
    "Iso_cal.fit(val_logits, val_labels)\n",
    "print(f\"Calibration terminée\")\n",
    "\n",
    "# 4. Calculer l'ECE après calibration\n",
    "calibrated_logits = Iso_cal(val_logits)\n",
    "probs_after = F.softmax(calibrated_logits, dim=1)\n",
    "ece_after = compute_ece(probs_after, val_labels)\n",
    "print(f\"ECE après calibration: {ece_after:.4f}\")\n",
    "\n",
    "# 5. Visualiser la calibration\n",
    "def plot_reliability_diagram(probs, labels, n_bins=10, title=\"\"):\n",
    "    confidences, predictions = torch.max(probs, 1)\n",
    "    accuracies = predictions.eq(labels)\n",
    "    \n",
    "    bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "    \n",
    "    accuracies_in_bins = []\n",
    "    confidences_in_bins = []\n",
    "    \n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
    "        prop_in_bin = in_bin.float().mean()\n",
    "        \n",
    "        if prop_in_bin.item() > 0:\n",
    "            accuracies_in_bins.append(accuracies[in_bin].float().mean().item())\n",
    "            confidences_in_bins.append(confidences[in_bin].mean().item())\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n",
    "    plt.plot(confidences_in_bins, accuracies_in_bins, 's-', label='Model')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Avant et après calibration\n",
    "plot_reliability_diagram(probs_before, val_labels, title=\"Avant calibration\")\n",
    "plot_reliability_diagram(probs_after, val_labels, title=\"Après calibration\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb93db3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
